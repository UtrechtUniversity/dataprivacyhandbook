# Pseudonymisation & Anonymisation {#pseudonymisation-anonymisation}

:::keywords
On this page: anonymous, pseudonymous, deidentification, safeguard, protection
measure, sdc, statistical disclosure control
:::

Pseudonymisation and anonymisation are both ways to make personal data less 
easily linkable to individual data subjects: they are methods to **de-identify**
personal data. Importantly, whereas anonymisation results in non-personal data 
that are not subject to the GDPR anymore, pseudonymised data are **still personal 
data**. It is therefore important to understand the difference between the two, 
and to estimate when your data are indeed fully anonymous.

:::warning
Any operation that you do up until the personal data are anonymised - including 
the anonymisation itself - is still subject to the GDPR. So even if you can
anonymise your data later, you still need to comply with the GDPR for everything
you do beforehand (e.g., collecting, analysing, sharing, etc.).
:::

In this chapter, we:

1. Explain what [pseudonymisation and anonymisation](#definition-anonymisation-pseudonymisation) 
mean.
2. Present a step-by-step 
[workflow to de-identify personal data](#deidentification-workflow).
3. List a number of [techniques](#deidentification-techniques) that you can use 
to de-identify personal data.

Finally, we list some resources for [further reading](#deidentification-resources).

## What are pseudonymisation and anonymisation? {#definition-anonymisation-pseudonymisation}

:::keywords
On this page: anonymous, pseudonymous, deidentification, safeguard, protection
measure, identifiable, sdc, statistical disclosure control, disclosure risk
:::

### Pseudonymisation {#pseudonymisation}

Pseudonymisation is a safeguard that reduces the linkability of your data to 
your data subjects ([rec. 28](https://gdpr-info.eu/recitals/no-28/){target="_blank"}). 
It means that you de-identify the data in such a way that they can no longer 
lead to identification *without additional information* 
([art. 4(5)](https://gdpr-info.eu/art-4-gdpr/){target="_blank"}). In theory, 
removing this additional information should lead to anonymised data. 

Pseudonymisation is often interpreted as replacing direct 
identifiers (e.g., names) with pseudonyms, and storing the link between the 
identifiers and the pseudonyms in a key file, separated from the research data. 
While this is a good practice (it makes sure that data are not directly 
identifiable anymore), this interpretation of pseudonymisation does not take 
into account [indirectly identifiable information](#when-anonymous), and thus 
does not necessarily fulfil the GDPR's definition of pseudonymisation!

:::fyi
Pseudonymous data are still **personal data** and thus subject to the GDPR. This 
is because the de-identification is *reversible*: identifying data subjects is 
still possible, just more difficult. This means that in order to use 
pseudonymous data, you still need to comply to all the rules in the GDPR.
:::

### Anonymisation {#anonymisation}

Anonymisation is a de-identification process that results in data that are 
"rendered anonymous in such a manner that the data subject is not or no longer 
identifiable" ([rec. 26](https://gdpr-info.eu/recitals/no-26/){target="_blank"}), 
neither directly nor indirectly, and by no one, including you. When data are 
anonymised, they are no longer personal data, and thus no longer subject to the 
GDPR. Note, however, that **everything you do *before* the data are anonymised** 
(including the anonymisation itself) *is* subject to the GDPR!

:::warning
Anonymisation is very difficult to accomplish in practice! 
[This video](https://www.youtube.com/watch?v=puQvpyf0W-M){target="_blank"} 
nicely illustrates why.
:::

### The identifiability spectrum {#identifiability-spectrum}

The relationship between (identifiable) personal data, pseudonymous data and 
anonymous data should be seen as lying on a spectrum. The more de-identified the 
data are, the closer they are to anonymous data and the lower the risk of 
re-identification. The visual guide below nicely illustrates this:

```{r identifiability-spectrum, echo=FALSE, out.width="100%"}
knitr::include_url("https://fpf.org/wp-content/uploads/2017/06/FPF_Visual-Guide-to-Practical-Data-DeID.pdf")
```

### When are data anonymous? {#when-anonymous}

Your data can be considered anonymous if data subjects can only be re-identified 
with an *unreasonable* amount of effort, i.e., taking into account the costs, 
required time and technology, and future technological developments 
([rec. 26](https://gdpr-info.eu/recitals/no-26/){target="_blank"}). 

Basically, your data are **not** anonymous (personal) when they comply with 
*any* of the [characteristics of personal data](#personal-data-assess):

- There is directly identifiable information (e.g., name, email address, social 
security number, etc.).
- Data subjects can be singled out (i.e., you can tell one data subject from 
another within a known group of data subjects).
- It is possible to identify data subjects by linking records ("mosaic effect"), 
either within your own database or when using other data sources. 
- It is possible to identify a data subject by inferring information about them 
(e.g., infer a disease by the variable "medication"), either within 
your own database or when using other data sources.
- It is possible to reverse the de-identification.

Whether data can be seen as anonymous strongly depends on the context of your 
research and how much information is available about the data subjects.

<img src="img/anonymous-chicken.PNG" title="Example of data that are not 
anonymous: it is possible to single out the brown chicken. Source: 
Hrudey et al., 2019, https://doi.org/10.5281/zenodo.3584841" 
alt="Comic about anonymous data. The left pane shows an animal that says 'Don't 
worry! We will only save general information about you, not anything that could 
identify you!', while holding a paper that says 'Brown chicken'. Next to the 
animal is a brown chicken giving a 'thumbs up'. The right pane shows that one 
brown chicken in a crowd of white chickens."/>

:::note
When collaborating with research data centres, such as the Statistics Netherlands
(Centraal Bureau voor de Statistiek, CBS), often 
[output checking guidelines](https://cros-legacy.ec.europa.eu/system/files/dwb_standalone-document_output-checking-guidelines.pdf){target="_blank"} 
are used to determine the risk of identification resulting from the analysis 
output of sensitive data. 
:::

### Alternatives to anonymisation {#anonymisation-alternatives}

Anonymisation is not the only solution. The best way to protect data subjects' 
privacy is to only collect/process their personal data if necessary 
([minimisation](#minimise)). Additionally, in many cases, full anonymisation 
is not even possible or desirable, for example if it results in too much 
information loss or incorrect inferences. 

If you cannot anonymise the data, there are always other ways in which you can 
protect the data, such as:

- De-identifying (pseudonymising) the data to the extent you can.
- Controlling access to the data, for example using 
[user agreements](#agreements), authentication, [encryption](#encryption), 
[secure analysis environments](#secure-computation), etc.
- Creating a [synthetic version](#synthetic-data) of your dataset to share with 
others.

## Step-by-step de-identification {#deidentification-workflow}

:::keywords
On this page: anonymous, pseudonymous, step-by-step, workflow, deidentification, 
safeguard, protection measure
:::

Below is a step-by-step workflow that you can use to de-identify your data. 
Whether or not the de-identification results in a pseudonymised or an anonymised 
dataset is highly dependent on the characteristics of the dataset and the context 
in which it was obtained.

1. Perform the de-identification in a **safe storage or processing environment**: 
remember that you are working with personal data, and as long as the data are 
not anonymous, they will be subject to the GDPR!<br><br>
2. Identify any **potentially identifying information** in your data.<br><br> 
3. Assess whether you need to **collect this information at all**. For example:
    a. Do you really need IP addresses in your survey data?
    b. Do you really need to record audio or video? 
    c. Do you really need a consent form with a name, contact information, and 
    signature on it?
    d. Replace names with pseudonyms in filenames and within the data where 
    possible.<br><br>
4. If you do not need **directly identifying information** to answer your research 
question, but you do need it to, for example, contact data subjects:
    a. Separate directly identifying information from the research data.
    b. Use pseudonyms to refer to individuals instead of names.
    c. Create a keyfile to link the pseudonyms to the names.
    d. Store the directly identifiable information and the keyfile in a separate 
    location from the research data and/or in encrypted form.<br><br>
5. Consider which types of information may lead to **indirect identification**, such 
as demographic information (age, education, occupation, etc.), geolocation, 
specific dates, medical conditions, unique personal characteristics, open text 
responses, etc.<br><br>
6. **De-identify** the directly and indirectly identifiable data using (a selection 
of) the [techniques described on the next page](#deidentification-techniques).
    a. Before you start, save a copy of the raw, untouched dataset, in case 
    anything in the process goes wrong.
    b. Document the steps you took, for example in a programming script or README 
    file, which always accompanies the data.
    c. Whether you can delete the raw (non-pseudonymised) version of the dataset, 
    depends on whether it needs to be preserved for verification purposes. Specific 
    restrictions may also apply if the Dutch Medical Research Involving Human 
    Subjects Act (WMO) and/or Good Clinical Practice apply to your research.<br><br>
7. **Treat the data according to their sensitivity**. If the data are not fully 
anonymised, they are pseudonymous and thus still need to be handled according to 
the GDPR guidelines!

:::note
How de-identified is de-identified enough? You can read more about this in the 
chapter [Statistical approaches to privacy](#statistical-privacy). 
:::

## De-identification techniques {#deidentification-techniques}

:::keywords
On this page: anonymous, pseudonymous, deidentification, safeguard, protection
measure, technique, anonymisation method, privacy-preserving, privacy-enhancing,
sdc, statistical disclosure control, disclosure risk
:::

Below is a list of techniques you can apply to your data to de-identify your 
dataset so that it results in a pseudonymised, or possibly even anonymised 
dataset. Bear in mind that applying these will always result in loss of 
information, so ask yourself how useful your dataset will still be after 
de-identification. 

The techniques are:

- [Suppression](#suppression)
- [Generalisation](#generalisation)
- [Replacement](#replacement)
- [Top- and bottom coding](#top-bottom-coding)
- [Adding noise](#adding-noise)
- [Permutation](#permutation)

:::note
**Statistical Disclosure Control (SDC)**<br>
The below de-identification methods are sometimes also referred to as methods 
to apply Statistical Disclosure Control (SDC). You will most likely encounter 
SDC when you collaborate with a research data centre such as Statistics 
Netherlands (Centraal Bureau voor de Statistiek, CBS). 
:::

#### Suppression {#suppression}

Suppression (sometimes called "masking") basically means removing variables, 
(parts of) values, or entire entries that you do not need from your dataset. 
Examples of data that you could consider removing:

- Name and contact information
- (Parts of) address
- Date, such as birthdate or participation date
- Social security number/Burgerservicenummer (BSN). NB. In the Netherlands, you 
are not allowed to use BSN in research at all!
- Medical record number
- IP address
- Facial features from neuroimaging data
- Automatically generated metadata such as GPS data in an image, author in a 
document, etc.
- Participants that form extreme outliers or are too unique

#### Generalisation {#generalisation}

Generalisation (also sometimes called abstraction, binning, aggregation, or 
categorisation) reduces the granularity of the data so that data subjects are 
less easily singled out. It can be applied to both qualitative (e.g., interview 
notes) and quantitative data (e.g., variables in a dataset). Here are some 
examples:

- Recoding date of birth into age.
- Categorising age into age groups.
- Recoding rare categories as “other” or as missing values.
- Replacing address with the name of a neighbourhood or town.
- Generalising specific persons in text into broader categories, e.g., "mother" 
to "[woman]", "Bob" to "[colleague]".
- Generalising specific locations into more general places, e.g., "Utrecht" to 
"[home town]", or from point coordinates to larger geographical areas (e.g., 
polygon or linear features).
- Coding open-ended responses into categories of responses, or as "responded" vs. 
"not responded".

#### Replacement {#replacement}

In this case, you replace sensitive details with non-sensitive ones, which are 
usually less informative, for example:

- Replacing directly identifying information that you do need with pseudonyms. 
When doing this, always store the key file securely and separately from the 
research data (e.g., use access control, [encryption](#encryption)). If you 
do not need the links with direct identifiers anymore, remove the keyfile or 
replace the pseudonyms with random identifiers without saving the key.
<details><summary>A good pseudonym:</summary>
<div>
  <ul>
    <li>Is not meaningful with respect to the data subjects: a random (unique) 
    number or string is better than a code that contains parts of personal 
    information, because the latter may reveal details about data subjects.</li>
    <li>Is managed securely, for example by appointing someone to be responsible 
    for managing access to the keyfile.</li>
    <li>Can be a simple number, random number, cryptographic hash function, text 
    string, etc. 
    ([read more](https://www.enisa.europa.eu/publications/pseudonymisation-techniques-and-best-practices){target="_blank"}).</li>
  </ul>
</div>
</details>

- Replacing identifiable text with "[redacted]". When redacting changes in-text, 
never just blank out the identifying value, always put a placeholder or 
pseudonym there, e.g., in `[`square brackets`]` or `<seg>`segments`</seg>`. 
- Replacing unique values with a summary statistic, e.g., the mean.
- Rounding values, making the data less precise.
- Replacing one or multiple variables with a hash.
<details><summary>What is hashing?</summary>
<div>
  Hashing is a way of obscuring data with a string of seemingly random 
  characters with a fixed length. It can be used to create a"hashed" pseudonym, or 
  to replace multiple variables with one unique value. There are many hash 
  functions which all have their own strength. It is usually quite difficult to 
  reverse the hashing process, except if an attacker has knowledge about the type 
  of information that was masked through hashing (e.g., for the MD5 algorithm, 
  there are many lookup tables that can reverse common hashes). To prevent 
  reversal, cryptographic hashing techniques add a "salt", i.e., a random number 
  or string, to the hash (the result is called a"digest"). If the "salt" is kept 
  confidential or is removed (similar to a keyfile), it is almost impossible to 
  reverse the hashing process.
</div>
</details>

#### Top- and bottom-coding {#top-bottom-coding}

Top- and bottom-coding are mostly useful for quantitative datasets that have 
some unique extreme values. It means that you set a maximum or minimum and 
recode all higher or lower values to that minimum or maximum. For example, you 
can top-code a variable "income" so that all incomes over €80.000 are set to 
€80.000. This does distort the distribution, but leaves a large part of the data 
intact.

#### Adding noise {#adding-noise}

[Adding noise](https://ec.europa.eu/justice/article-29/documentation/opinion-recommendation/files/2014/wp216_en.pdf#page=12){target="_blank"} 
to data obfuscates sensitive details. It is mostly applied to quantitative 
datasets, but can also apply to other types of data. For example:

- Adding half a standard deviation to a variable.
- Multiplying a variable by a random number.
- Applying [Differential Privacy](#differential-privacy) guarantees to an algorithm.
- Blurring (pixelating) images and videos.
- Voice alteration in audio. 

#### Permutation {#permutation}

[Permutation](https://ec.europa.eu/justice/article-29/documentation/opinion-recommendation/files/2014/wp216_en.pdf#page=13){target="_blank"} 
means swapping values between data subjects, so that it becomes more difficult 
to link information belonging to one data subject together. This will keep the 
distribution and summary statistics constant, but change correlations between 
variables, making some statistical analyses more difficult or impossible.

## Tools and further reading {#deidentification-resources}

:::keywords
On this page: anonymous, pseudonymous, deidentification, safeguard, protection
measure, tool, resource, reading material
:::

You can find a selection of **de-identification tools** in 
[this GitHub repository](https://github.com/UtrechtUniversity/privacy-engineering-tools/tree/main/deidentification){target="_blank"}.

For further reading, we compiled a reading list on this topic in our publicly 
accessible [Zotero library](https://www.zotero.org/groups/2554340/data_privacy_uu/collections/6HS9MPBR){target="_blank"}. 

We can recommend:

- [10 misunderstandings related to anonymisation](https://edps.europa.eu/system/files/2021-04/21-04-27_aepd-edps_anonymisation_en_5.pdf){target="_blank"}. 
- [Risk management for research data about people](https://doi.org/10.5281/zenodo.3584332){target="_blank"}. 
- CESSDA’s [Data Management Expert Guide on Anonymisation](https://dmeg.cessda.eu/Data-Management-Expert-Guide/5.-Protect/Anonymisation){target="_blank"}.
- [Anonymisation: managing data protection risk, code of practice](https://ico.org.uk/media/1061/anonymisation-code.pdf){target="_blank"}. 
- [Privacy protection in the era of open science](https://doi.org/10.31234/osf.io/ybzu9){target="_blank"}.
- Statistics Netherlands (CBS) on 
[techniques used to de-identify sensitive data](https://www.cbs.nl/en-gb/our-services/methods/statistical-methods/output/statistical-disclosure-control){target="_blank"}.